<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.65.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>AGSS-VOS: Attention Guided Single-Shot Video Object Segmentation 论文解读&nbsp;&ndash;&nbsp;Cynicsss</title><link rel="stylesheet" href="/css/core.min.1de5bf910dc5f928da803dd141aaa30c2108d9beed6bcf138e6100d583ff3d0ed3cc6c854b913d26bc4e3b1608f31f53.css" integrity="sha384-HeW/kQ3F&#43;SjagD3RQaqjDCEI2b7ta88TjmEA1YP/PQ7TzGyFS5E9JrxOOxYI8x9T"><body>
    <div class="base-body"><section id="header" class="site header max-body-width">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/images/image.png" alt /><span class="site name">Cynicsss</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="/categories/">Categories</a><a class="nav item" href="/tags/">Tags</a><a class="nav item" href="/post/">Posts</a><a class="nav item" href="/about/about">About</a></nav></div></span></div><div class="site slogan"><span class="title">Computer Vision, Postgraduate</span></div></section><div id="content" class="max-body-width"><section class="article header">
    <h1 class="article title">AGSS-VOS: Attention Guided Single-Shot Video Object Segmentation 论文解读</h1><p class="article date">Jan 29, 2020</p></section><article class="article markdown-body"><p>此篇文章为iccv2019中关于视频分割的一篇文章，主要针对多物体进行视频object分割，值得一读</p>
<h2 id="agss-vos">AGSS-VOS</h2>
<p>论文地址：<br>
<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_AGSS-VOS_Attention_Guided_Single-Shot_Video_Object_Segmentation_ICCV_2019_paper.pdf"target="_blank">AGSS-VOS: Attention Guided Single-Shot Video Object Segmentation</a></p>
<p>大多数的视频分割方法每次只能处理一个object，当一个视频序列需要分割多个object时，这种方法就会非常耗时。此篇文章作者提出一个方法，只用一次前向传播，经过模糊实例和细分实例两个模块，将多个object进行一次性分割出来，整个网络框架是端到端的。作者在论文开头放了一张实验效果图，是与<strong>RGMP</strong>(一次传播只分割一个物体的典型方法)做了对比，如下图：</p>
<p><a target="_blank" rel="noopener noreferrer" 
  href="/images/AGSS-VOS-01.png"><img  src="/images/AGSS-VOS-01.png"
        alt="Figure 1"/></a></p>
<p>可以看到，当视频物体数量上升的时候<strong>RGMP</strong>的推理时间在不断上升，而本文的方法依旧处在一个比较快速的水平，不会因物体的数量而大幅度影响推理速度。我们下面来看一下本篇文章的网络框架：</p>
<h3 id="网络框架">网络框架</h3>
<p><a target="_blank" rel="noopener noreferrer" 
  href="/images/AGSS-VOS-02.png"><img  src="/images/AGSS-VOS-02.png"
        alt="Figure 2"/></a></p>
<p>整个框架，初看非常复杂，不知道从哪里入手，这主要是因为输入比较多，一旦把输入一个一个理清楚，后面的网络部分是非常简单的。此方法与<strong>RGMP</strong>一样，输入一共有三帧，预测帧，预测的前一帧以及初始帧，分别为 <strong>It, It-1, I0</strong>。同时，也加入了前一帧以及初始帧的mask。</p>
<h4 id="instance-agnostic-module">Instance-Agnostic Module</h4>
<p>整个网络分为三大块，首先介绍<strong>Instance-Agnostic Module</strong>这一模块，此模块的输入有两个，一个是初试帧的image及mask的叠加，另一个是预测帧及预测前一帧mask的扭曲的叠加。这两个mask都是与instance无关的，也就是每一个instance全都混在一起取一个值，预测前一帧mask的扭曲是根据当前帧及前一帧得到的光流操作后得到的。两个image+mask经过一个孪生网络，然后concat起来，会得到一个与instance无关的，相当于是只分出前景的attention。</p>
<h4 id="instance-specific-module">Instance-Specific Module</h4>
<p>此模块的输入是当前帧image与扭曲mask的叠加，同时若有N个object，就会有N个这样的叠加，每一个代表一个object。每一个输入都经过一个轻型Encoder，输出再与扭曲mask再concat一次，经过一个<strong>Attention Generator</strong>生成attention，如图中的(d)。通过这个模块就可以得到每一个object单独的预测。</p>
<h4 id="attention-guided-decoder">Attention-Guided Decoder</h4>
<p>在这一模块中，把两个模块的输出进行点乘，就会得到每个object的最终预测了。有了最终预测之后还需要一次Normalize，这一步主要是因为预测会有重叠部分，而一个像素只能代表一个object所以需要有一个操作来进行重叠区域的处理，normalize的公式如下：</p>
<p><a target="_blank" rel="noopener noreferrer" 
  href="/images/AGSS-VOS-03.png"><img  src="/images/AGSS-VOS-03.png"
        alt="Figure 3"/></a></p>
<h3 id="training-loss">Training Loss</h3>
<p>Loss部分作者使用了IoU Loss：</p>
<p><a target="_blank" rel="noopener noreferrer" 
  href="/images/AGSS-VOS-04.png"><img  src="/images/AGSS-VOS-04.png"
        alt="Figure 4"/></a></p>
<p>训练的具体细节可以去原文中看，在这里不再赘述。</p>
<h3 id="experiments">Experiments</h3>
<p>作者在 <strong>Youtube VOS</strong> 和 <strong>DAVIS-2017</strong> 上进行了实验 效果都很好。
<a target="_blank" rel="noopener noreferrer" 
  href="/images/AGSS-VOS-05.png"><img  src="/images/AGSS-VOS-05.png"
        alt="Figure 5"/></a>
<a target="_blank" rel="noopener noreferrer" 
  href="/images/AGSS-VOS-06.png"><img  src="/images/AGSS-VOS-06.png"
        alt="Figure 6"/></a></p></article><section class="article labels"><a class="article category" href=/categories/computer-vision/><span class="hashtag">#</span>Computer Vision</a><a class="article tag" href=/tags/video-object-segmentation/><span class="hashtag">#</span>Video Object Segmentation</a></section><section class="article navigation"><p><a class="link" href="/post/%E5%AE%9E%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="li"></span>实用的数据结构</a class="link">
    </p></section></div><section id="footer" class="footer max-body-width"><div class="footer-wrap">
    <p class="copyright">©2019 Cynicsss.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div></section></div>
</body>

</html>